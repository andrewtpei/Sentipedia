{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<font style='font-size:1.5em'>**Initial Data Analysis**</font>\n",
                "\n",
                "<font style='font-size:1.2em'>Using Reddit CSV</font>\n",
                "\n",
                "**Author: Sentipedia**  \n",
                "\n",
                "**Course: DS105L Project** \n",
                "\n",
                "**DATE: 3/29/2023** \n",
                "\n",
                "---\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Imports "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Importing Libraries "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "# importing libraries for web scrapping and API acess\n",
                "import requests\n",
                "from bs4 import BeautifulSoup\n",
                "\n",
                "# importing libraries for dataframes \n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "# importing libraries for plots \n",
                "from plotnine import *\n",
                "\n",
                "# importing libraries for ploting networks\n",
                "import networkx as nx\n",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Data"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Looking at the Data "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[nltk_data] Downloading package punkt to /Users/ap/nltk_data...\n",
                        "[nltk_data]   Package punkt is already up-to-date!\n",
                        "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
                        "[nltk_data]     /Users/ap/nltk_data...\n",
                        "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
                        "[nltk_data]       date!\n",
                        "[nltk_data] Downloading package vader_lexicon to\n",
                        "[nltk_data]     /Users/ap/nltk_data...\n",
                        "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "True"
                        ]
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# import language model \n",
                "import nltk\n",
                "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
                "\n",
                "# downloading NLTK analyzers \n",
                "nltk.download('punkt')\n",
                "nltk.download('averaged_perceptron_tagger')\n",
                "nltk.download('vader_lexicon')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>comment</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>Agree.  With the Tories pushing Brexit, they h...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>I just thought that with plenty of vaccination...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>If you don't listen to your core voter base, t...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>Much as I agree with your potential projection...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>Brexit, the word, is why it succeeded. Nobody ...</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "                                             comment\n",
                            "0  Agree.  With the Tories pushing Brexit, they h...\n",
                            "1  I just thought that with plenty of vaccination...\n",
                            "2  If you don't listen to your core voter base, t...\n",
                            "3  Much as I agree with your potential projection...\n",
                            "4  Brexit, the word, is why it succeeded. Nobody ..."
                        ]
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "df_reddit = pd.read_csv('reddit.csv', usecols=['comment'])\n",
                "df_reddit.head()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Using NLTK"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>comment</th>\n",
                            "      <th>pos</th>\n",
                            "      <th>score</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>Agree.  With the Tories pushing Brexit, they h...</td>\n",
                            "      <td>[(Agree, NNP), (., .), (With, IN), (the, DT), ...</td>\n",
                            "      <td>{'neg': 0.22, 'neu': 0.606, 'pos': 0.174, 'com...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>I just thought that with plenty of vaccination...</td>\n",
                            "      <td>[(I, PRP), (just, RB), (thought, VBD), (that, ...</td>\n",
                            "      <td>{'neg': 0.063, 'neu': 0.705, 'pos': 0.232, 'co...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>If you don't listen to your core voter base, t...</td>\n",
                            "      <td>[(If, IN), (you, PRP), (do, VBP), (n't, RB), (...</td>\n",
                            "      <td>{'neg': 0.025, 'neu': 0.959, 'pos': 0.016, 'co...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>Much as I agree with your potential projection...</td>\n",
                            "      <td>[(Much, JJ), (as, IN), (I, PRP), (agree, VBP),...</td>\n",
                            "      <td>{'neg': 0.053, 'neu': 0.871, 'pos': 0.076, 'co...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>Brexit, the word, is why it succeeded. Nobody ...</td>\n",
                            "      <td>[(Brexit, NN), (,, ,), (the, DT), (word, NN), ...</td>\n",
                            "      <td>{'neg': 0.0, 'neu': 0.776, 'pos': 0.224, 'comp...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>...</th>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>995</th>\n",
                            "      <td>Exactly, many interpreted it as a vote for a \u0018...</td>\n",
                            "      <td>[(Exactly, RB), (,, ,), (many, JJ), (interpret...</td>\n",
                            "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>996</th>\n",
                            "      <td>Just because parties committed to implimenting...</td>\n",
                            "      <td>[(Just, RB), (because, IN), (parties, NNS), (c...</td>\n",
                            "      <td>{'neg': 0.026, 'neu': 0.762, 'pos': 0.212, 'co...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>997</th>\n",
                            "      <td>I understand that but to persuade the EU27 tha...</td>\n",
                            "      <td>[(I, PRP), (understand, VBP), (that, DT), (but...</td>\n",
                            "      <td>{'neg': 0.0, 'neu': 0.906, 'pos': 0.094, 'comp...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>998</th>\n",
                            "      <td>&amp;gt; The whole partnership is damaged and the ...</td>\n",
                            "      <td>[(&amp;, CC), (gt, NN), (;, :), (The, DT), (whole,...</td>\n",
                            "      <td>{'neg': 0.067, 'neu': 0.74, 'pos': 0.193, 'com...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>999</th>\n",
                            "      <td>Speaking as a 48%-er, no. If we cancelled brex...</td>\n",
                            "      <td>[(Speaking, VBG), (as, IN), (a, DT), (48, CD),...</td>\n",
                            "      <td>{'neg': 0.145, 'neu': 0.855, 'pos': 0.0, 'comp...</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "<p>1000 rows Ã— 3 columns</p>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "                                               comment  \\\n",
                            "0    Agree.  With the Tories pushing Brexit, they h...   \n",
                            "1    I just thought that with plenty of vaccination...   \n",
                            "2    If you don't listen to your core voter base, t...   \n",
                            "3    Much as I agree with your potential projection...   \n",
                            "4    Brexit, the word, is why it succeeded. Nobody ...   \n",
                            "..                                                 ...   \n",
                            "995  Exactly, many interpreted it as a vote for a \u0018...   \n",
                            "996  Just because parties committed to implimenting...   \n",
                            "997  I understand that but to persuade the EU27 tha...   \n",
                            "998  &gt; The whole partnership is damaged and the ...   \n",
                            "999  Speaking as a 48%-er, no. If we cancelled brex...   \n",
                            "\n",
                            "                                                   pos  \\\n",
                            "0    [(Agree, NNP), (., .), (With, IN), (the, DT), ...   \n",
                            "1    [(I, PRP), (just, RB), (thought, VBD), (that, ...   \n",
                            "2    [(If, IN), (you, PRP), (do, VBP), (n't, RB), (...   \n",
                            "3    [(Much, JJ), (as, IN), (I, PRP), (agree, VBP),...   \n",
                            "4    [(Brexit, NN), (,, ,), (the, DT), (word, NN), ...   \n",
                            "..                                                 ...   \n",
                            "995  [(Exactly, RB), (,, ,), (many, JJ), (interpret...   \n",
                            "996  [(Just, RB), (because, IN), (parties, NNS), (c...   \n",
                            "997  [(I, PRP), (understand, VBP), (that, DT), (but...   \n",
                            "998  [(&, CC), (gt, NN), (;, :), (The, DT), (whole,...   \n",
                            "999  [(Speaking, VBG), (as, IN), (a, DT), (48, CD),...   \n",
                            "\n",
                            "                                                 score  \n",
                            "0    {'neg': 0.22, 'neu': 0.606, 'pos': 0.174, 'com...  \n",
                            "1    {'neg': 0.063, 'neu': 0.705, 'pos': 0.232, 'co...  \n",
                            "2    {'neg': 0.025, 'neu': 0.959, 'pos': 0.016, 'co...  \n",
                            "3    {'neg': 0.053, 'neu': 0.871, 'pos': 0.076, 'co...  \n",
                            "4    {'neg': 0.0, 'neu': 0.776, 'pos': 0.224, 'comp...  \n",
                            "..                                                 ...  \n",
                            "995  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...  \n",
                            "996  {'neg': 0.026, 'neu': 0.762, 'pos': 0.212, 'co...  \n",
                            "997  {'neg': 0.0, 'neu': 0.906, 'pos': 0.094, 'comp...  \n",
                            "998  {'neg': 0.067, 'neu': 0.74, 'pos': 0.193, 'com...  \n",
                            "999  {'neg': 0.145, 'neu': 0.855, 'pos': 0.0, 'comp...  \n",
                            "\n",
                            "[1000 rows x 3 columns]"
                        ]
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "df_reddit_nltk = df_reddit.head(1000).copy()\n",
                "\n",
                "analyzer = SentimentIntensityAnalyzer()\n",
                "i = 0\n",
                "df_reddit_nltk['pos'] = None\n",
                "df_reddit_nltk['score'] = None\n",
                "\n",
                "for comment in df_reddit_nltk['comment']:\n",
                "     tokens = nltk.word_tokenize(df_reddit_nltk['comment'][i])\n",
                "     df_reddit_nltk['pos'][i] = nltk.pos_tag(tokens)\n",
                "     df_reddit_nltk['score'][i] = analyzer.polarity_scores(df_reddit_nltk['comment'][i])\n",
                "     i = i + 1\n",
                "\n",
                "df_reddit_nltk"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Using DistilBERT Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight']\n",
                        "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
                        "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
                        "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
                        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
                "\n",
                "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
                "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'LABEL_1'"
                        ]
                    },
                    "execution_count": 17,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
                "with torch.no_grad():\n",
                "    logits = model(**inputs).logits\n",
                "\n",
                "predicted_class_id = logits.argmax().item()\n",
                "model.config.id2label[predicted_class_id]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>comment</th>\n",
                            "      <th>predicted</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>Agree.  With the Tories pushing Brexit, they h...</td>\n",
                            "      <td>LABEL_1</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>I just thought that with plenty of vaccination...</td>\n",
                            "      <td>LABEL_1</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>If you don't listen to your core voter base, t...</td>\n",
                            "      <td>LABEL_1</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>Much as I agree with your potential projection...</td>\n",
                            "      <td>LABEL_1</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>Brexit, the word, is why it succeeded. Nobody ...</td>\n",
                            "      <td>LABEL_1</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>5</th>\n",
                            "      <td>\"Brexit means Brexit\"... =\"\\n\\nThe one stand o...</td>\n",
                            "      <td>LABEL_1</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>6</th>\n",
                            "      <td>The word actually wasn\u0019t really used much anyw...</td>\n",
                            "      <td>LABEL_1</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>7</th>\n",
                            "      <td>It was used, but only as a kind of novelty ter...</td>\n",
                            "      <td>LABEL_1</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>8</th>\n",
                            "      <td>This is exact right - the portmanteau of \u0018Grex...</td>\n",
                            "      <td>LABEL_1</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>9</th>\n",
                            "      <td>It\u0019s probably somewhere between the two of us ...</td>\n",
                            "      <td>LABEL_1</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "                                             comment predicted\n",
                            "0  Agree.  With the Tories pushing Brexit, they h...   LABEL_1\n",
                            "1  I just thought that with plenty of vaccination...   LABEL_1\n",
                            "2  If you don't listen to your core voter base, t...   LABEL_1\n",
                            "3  Much as I agree with your potential projection...   LABEL_1\n",
                            "4  Brexit, the word, is why it succeeded. Nobody ...   LABEL_1\n",
                            "5  \"Brexit means Brexit\"... =\"\\n\\nThe one stand o...   LABEL_1\n",
                            "6  The word actually wasn\u0019t really used much anyw...   LABEL_1\n",
                            "7  It was used, but only as a kind of novelty ter...   LABEL_1\n",
                            "8  This is exact right - the portmanteau of \u0018Grex...   LABEL_1\n",
                            "9  It\u0019s probably somewhere between the two of us ...   LABEL_1"
                        ]
                    },
                    "execution_count": 20,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "df_reddit_DistilBERT = df_reddit.head(10).copy()\n",
                "i = 0\n",
                "df_reddit_DistilBERT['predicted'] = None\n",
                "\n",
                "for comment in df_reddit_DistilBERT['comment']:\n",
                "     inputs = tokenizer(df_reddit_DistilBERT['comment'][i], return_tensors=\"pt\")\n",
                "     with torch.no_grad():\n",
                "          logits = model(**inputs).logits\n",
                "\n",
                "     predicted_class_id = logits.argmax().item()\n",
                "     df_reddit_DistilBERT['predicted'][i] = model.config.id2label[predicted_class_id]\n",
                "     i = i + 1\n",
                "\n",
                "df_reddit_DistilBERT"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "text = \"This is an example sentence to encode using DistilBERT.\"\n",
                "encoded_text = tokenizer.encode(\"[CLS] \" + text + \" [SEP]\")\n",
                "input_ids = torch.tensor([encoded_text])\n",
                "outputs = model(input_ids)\n",
                "hidden_states = outputs[0]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[  101,   101,  2023,  2003,  2019,  2742,  6251,  2000,  4372, 16044,\n",
                            "          2478,  4487, 16643, 23373,  1012,   102,   102]])"
                        ]
                    },
                    "execution_count": 13,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "input_ids"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "SequenceClassifierOutput(loss=None, logits=tensor([[-0.1000, -0.0733]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
                        ]
                    },
                    "execution_count": 14,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "outputs"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.13"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
