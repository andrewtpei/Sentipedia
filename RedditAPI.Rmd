```{r}
#Packages: 
library(jsonlite);library(RedditExtractoR);library(tidyverse);library(purrr);library(tidytext); library(wordcloud2); library(textdata); library(ggthemes); library(syuzhet); library(widyr)
```

OBTAINING AND FILTERING DATA: 
```{r}
##DO NOT RUN UNLESS WANT TO WAIT FOR AN HOUR
#Returns a dataframe with the threads and basic info (e.g comments, date of post, content of post, and crucially the comment url)
top_Brexit_urls <- find_thread_urls(subreddit = "unitedkingdom", keywords = "Brexit", sort_by = "top", period = "all");
top_Brexit_urls$date_utc <- as.Date(top_Brexit_urls$date_utc)
Atop_Brexit_urls <- arrange(top_Brexit_urls, date_utc)
dim(Atop_Brexit_urls); summary(Atop_Brexit_urls); head(Atop_Brexit_urls)
#Returns the comments within the threads 
threads_contents <- get_thread_content(Atop_Brexit_urls$url)
df_Brexit <- threads_contents[["comments"]] %>% select(-c("author", "upvotes", "downvotes")) %>% filter(!str_detect(comment, regex("deleted", ignore_case = TRUE)))

```

```{r}
#Cleaning: 
dim(df_Brexit); summary(df_Brexit); head(df_Brexit)
filtered <- df_Brexit %>% filter(str_detect(comment, regex("Brexit", ignore_case = TRUE)));dim(filtered); summary(filtered)
summary(filtered)

```

How are people describing Brexit over time? 
```{r}
##Wordclouds Graveyard
text <- filtered %>% tibble() %>% unnest_tokens(word,comment) %>% left_join(parts_of_speech) %>% filter(!is.na(pos)) %>% filter(pos %in% c("Adjective","Verb")) %>% anti_join(stop_words, by = "word") %>% anti_join(dfcustomstop_words, by = "word") %>% left_join(sentiments, by = "word") %>%
  filter(!is.na(value) & value != 0) %>% arrange(timestamp) %>% count(word, sort = T) %>% rename(freq = n)
```

```{r}
#Words to filter out
word <- c("British", "pro", "left", "right", "anti", "leave","hard", "yeah")
dfcustomstop_words <- as.data.frame(word)
sentiments <- get_sentiments("afinn")

texttopic <- df %>% tibble() %>% unnest_tokens(word,comment) %>% left_join(parts_of_speech) %>% filter(!pos %in% c("Adjective","Verb", "Adverb", "Verb (transitive)", "Verb (intransitive)")) %>% anti_join(stop_words, by = "word") %>% arrange(timestamp)

n <- nrow(texttopic)
texttopic1 <- texttopic[1:round(n/4), ]
texttopic2 <- texttopic[(round(n/4) + 1):round(n/2), ]
texttopic3 <- texttopic[(round(n/2) + 1):round(3*n/4), ]
texttopic4 <- texttopic[(round(3*n/4) + 1):n, ]

set.seed(123)
cloudtopic1 <- texttopic1 %>% count(word, sort = T) %>% rename(freq = n);cloudtopic2 <- texttopic2 %>% count(word, sort = T) %>% rename(freq = n);cloudtopic3 <- texttopic3 %>% count(word, sort = T) %>% rename(freq = n);cloudtopic4 <- texttopic4 %>% count(word, sort = T) %>% rename(freq = n)
wordcloud2(cloudtopic1, shape = "circle");wordcloud2(cloudtopic2, shape = "circle"); wordcloud2(cloudtopic3, shape = "circle"); wordcloud2(cloudtopic4, shape = "circle")
```

```{r}
#Average Sentiment of words over time
toptopic1 <- texttopic %>% count(word, sort = T) %>% dplyr::rename(freq = n)
toptopic2 <- texttopic %>% count(word, sort = T) %>% dplyr::rename(freq = n)
toptopic3 <- texttopic %>% count(word, sort = T) %>% dplyr::rename(freq = n)
toptopic4 <- texttopic %>% count(word, sort = T) %>% dplyr::rename(freq = n)
```

```{r}
##Word Clouds Emotions
word <- c("British", "pro", "left", "right", "anti", "leave","hard", "yeah", "Brexit"); dfcustomstop_words <- as.data.frame(word)
sentiments <- get_sentiments("afinn")

text <- df %>% tibble() %>% unnest_tokens(word,comment) %>% left_join(sentiments, by = "word") %>% filter(!is.na(value) & value != 0) %>% anti_join(stop_words, by = "word") %>% anti_join(dfcustomstop_words, by = "word") %>% arrange(timestamp)

n <- nrow(text)
text1 <- text[1:round(n/4), ]
text2 <- text[(round(n/4) + 1):round(n/2), ]
text3 <- text[(round(n/2) + 1):round(3*n/4), ]
text4 <- text[(round(3*n/4) + 1):n, ]

cloud1 <- text1 %>% count(word, sort = T) %>% rename(freq = n);cloud2 <- text2 %>% count(word, sort = T) %>% rename(freq = n);cloud3 <- text3 %>% count(word, sort = T) %>% rename(freq = n);cloud4 <- text4 %>% count(word, sort = T) %>% rename(freq = n)
wordcloud2(cloud1, shape = "circle");wordcloud2(cloud2, shape = "circle"); wordcloud2(cloud3, shape = "circle"); wordcloud2(cloud4, shape = "circle")
```

```{r}
#Average Sentiment of words over time
top1 <- text1 %>% count(word, sort = T) %>% dplyr::rename(freq = n) %>% left_join(sentiments, by = "word") %>% mutate(total = freq*value); mean(sum(top1$total)/sum(top1$freq))
top2 <- text2 %>% count(word, sort = T) %>% dplyr::rename(freq = n) %>% left_join(sentiments, by = "word") %>% mutate(total = freq*value); mean(sum(top2$total)/sum(top2$freq))
top3 <- text3 %>% count(word, sort = T) %>% dplyr::rename(freq = n) %>% left_join(sentiments, by = "word") %>% mutate(total = freq*value); mean(sum(top3$total)/sum(top3$freq))
top4 <- text4 %>% count(word, sort = T) %>% dplyr::rename(freq = n) %>% left_join(sentiments, by = "word") %>% mutate(total = freq*value); mean(sum(top4$total)/sum(top4$freq))
```

```{r}
##########################################################################
##########################################################################
#Create bigram network diagrams 
##########################################################################
##########################################################################
```

```{r}
df_bigrams <- df %>%
  unnest_tokens(bigram, comment, token = "ngrams", n = 2) %>%
  filter(!is.na(bigram))
bigrams_separated <- df_bigrams %>% separate(bigram, c("word1", "word2"), sep = " ")
custom_stopwords <- c("http", "amp", "https", "gt", "xit", "news")
```

```{r}
#Just emotion
bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>% 
  filter(word1 %in% sentiments$word) %>% 
  filter(word2 %in% sentiments$word) %>% 
  filter(!word1 %in% custom_stopwords) %>% 
  filter(!word2 %in% custom_stopwords) 
  

bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")
```

```{r}
#Just emotion
bigram_graph <- bigrams_united %>% count(bigram, sort = T) %>% filter(n > 10) %>% graph_from_data_frame()

set.seed(123)
bidiagram<- ggraph(bigram_graph, layout = "fr") +
        geom_edge_link() +
        geom_node_point(color = "lightblue", size = 5) +
        geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
        theme_void()
bidiagram
```

```{r}
#Including all
bigrams_filtered_all <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>% 
  filter(!word1 %in% custom_stopwords) %>% 
  filter(!word2 %in% custom_stopwords) 

bigrams_united_all <- bigrams_filtered_all %>%
  unite(bigram, word1, word2, sep = " ")
```

```{r}
#Including all
bigram_graph_all <- bigrams_united_all %>% count(bigram, sort = T) %>% filter(n > 150) %>% graph_from_data_frame()

set.seed(123)
bidiagram_all<- ggraph(bigram_graph_all, layout = "fr") +
        geom_edge_link() +
        geom_node_point(color = "lightblue", linewidth = 1) +
        geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
        theme_void()
bidiagram_all
```


```{r}
#Sentiment over time 
filtered$date <- as.Date(filtered$date)
timesent<- cbind(time = filtered$date, sent = sentiment$ave_sentiment)
df_timesent <- timesent %>% as.data.frame() %>% mutate(sent = as.numeric(sent))

df_summarised <- df_timesent %>% group_by(time) %>% summarise(sent = mean(sent))

timesentplot <- ggplot(df_summarised, aes(x = time, y = sent)) +
  geom_point(size = 2) +
  geom_smooth(method = "lm", se = TRUE, level = 0.99, alpha = 0.7, size = 1.5) +
  scale_fill_gradient(low="blue", high="red") +
  labs(title = "Sentiment Over Time", x = "Time (Day)", y = "Average Comment Sentiment (-4 to 4)") 

timesentplot + theme_minimal() +
  theme(plot.title = element_text(size = 20, face = "bold"),
        axis.title = element_text(size = 15),
        axis.text = element_text(size = 15),
        legend.position = "none",
        panel.grid.major = element_line(color = "grey", linetype = "dashed"),
        panel.grid.minor = element_blank(),
        panel.background = element_rect(fill = "grey90"),
        plot.caption = element_text(size = 10, hjust = 0)) + theme(axis.text.x = element_blank())
```

```{r}
##########################################################################
##########################################################################
#Word Combination
##########################################################################
##########################################################################
```

```{r}
#Word Correlation: 
df_wordcorr <- df %>% tibble() %>% mutate(document = 1:nrow(df)) %>% unnest_tokens(word,comment) %>% left_join(sentiments, by = "word") %>% filter(value != 0 | word == "brexit" | word == "referendum" | word == "government" | word == "labour" | word == "tory" | word == "economy" | word == "politicians" | word == "ireland" | word == "scotland" | word == "hard") %>% anti_join(stop_words, by = "word") 

word_corr <- df_wordcorr %>% widyr::pairwise_cor(word, document, sort = TRUE) 

brexit <- word_corr %>% filter(item1 == "brexit" | item1 == "referendum" | item1 == "government" | item1 == "labour" | item1 == "tory" | item1 == "economy" | item1 == "politicians" | item1 == "ireland" | item1 == "scotland" | item1 == "hard") %>% filter(n > 100) %>% graph_from_data_frame()

brexit <- word_pairs %>% filter(item1 == "brexit" | item1 == "referendum" | item1 == "government" | item1 == "labour" | item1 == "tory" | item1 == "economy" | item1 == "politicians" | item1 == "ireland" | item1 == "scotland" | item1 == "hard") %>% filter(n > 100) %>% graph_from_data_frame()

brexit2 <- word_pairs %>% filter(item1 == "brexit") %>% graph_from_data_frame()

brexit2 <- word_pairs %>% filter(item1 == "brexit") %>% graph_from_data_frame()

set.seed(123)
a <- grid::arrow(type = "closed", length = unit(.05, "inches"))
b<- ggraph(government, layout = "fr") +
        geom_edge_link() +
        geom_node_point(color = "lightblue", size = 1) +
        geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
        theme_void()

```

brexit \<- word_pairs %\>% filter(item1 == "brexit" \| item1 == "referendum" \| item1 == "government" \| item1 == "labour" \| item1 == "tory" \| item1 == "economy" \| item1 == "politicians" \| item1 == "ireland" \| item1 == "scotland" \| item1 == "hard") %\>% filter(n \> 150) %\>% graph_from_data_frame()

filter(value != 0 \| word == "brexit" \| word == "government" \| word == "labour" \| word == "tory" \| word == "economy" \| word == "politicians" \| word == "ireland" \| word == "scotland" \| word == "hard") %\>% anti_join(stop_words, by = "word")

```{r}
##Word Count Diagram with Brexit and Brexit-related topics
df_wordcorr <- df %>% tibble() %>% mutate(document = 1:nrow(df)) %>% unnest_tokens(word,comment) %>% left_join(sentiments, by = "word") %>% filter(value != 0 | word == "brexit" | word == "government" | word == "labour" | word == "tory" | word == "economy" | word == "politicians" | word == "ireland" | word == "scotland" | word == "hard") %>% anti_join(stop_words, by = "word") 

word_pairs <- df_wordcorr %>% widyr::pairwise_count(word, document, sort = TRUE) 

brexit <- word_pairs %>% filter(n > 100) %>% graph_from_data_frame()

set.seed(123)
brexittopicgraph<- ggraph(brexit, layout = "fr") +
        geom_edge_link(alpha = 0.2) +
        geom_node_point(color = "lightblue", size = 3) +
        geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
        theme_void()
brexittopicgraph
```

TIME GRAPH OF EMOTIONS
```{r}
#Naive Emotional Classification
filteredtest <- filtered %>% arrange(timestamp) %>% sentimentr::get_sentences()
filteredemotion <- filtered %>% arrange(timestamp) %>% sentimentr::get_sentences() %>% sentimentr::emotion_by() 
filteredemotion <- filtered %>% arrange(timestamp) %>% sentimentr::get_sentences() %>% get_nrc_sentiment() %>%

```

'
```{r}
#Naive Emotional Classification
library(sentimentr)
filteredtime <- filtered %>% arrange(timestamp)
n <- nrow(filteredtime)
filteredtime1 <- filteredtime[1:round(n/4), ]
filteredtime2 <- filteredtime[(round(n/4) + 1):round(n/2), ]
filteredtime3 <- filteredtime[(round(n/2) + 1):round(3*n/4), ]
filteredtime4 <- filteredtime[(round(3*n/4) + 1):n, ]

sentences1 <- sentimentr::get_sentences(filteredtime1$comment)
sentences2 <- sentimentr::get_sentences(filteredtime2$comment)
sentences3 <- sentimentr::get_sentences(filteredtime3$comment)
sentences4 <- sentimentr::get_sentences(filteredtime4$comment)

emotion1<- sentimentr::emotion_by(sentences1)
emotion2<- sentimentr::emotion_by(sentences2)
emotion3<- sentimentr::emotion_by(sentences3)
emotion4<- sentimentr::emotion_by(sentences4)

emotionfilter1 <- emotion1 %>% filter(emotion_count != 0) %>% group_by(emotion_type) %>% dplyr::summarise(emotion_count = n()) %>% arrange(-emotion_count)

emotionfilter2 <- emotion2 %>% filter(emotion_count != 0) %>% group_by(emotion_type) %>% dplyr::summarise(emotion_count = n()) %>% arrange(-emotion_count)

emotionfilter3 <- emotion3 %>% filter(emotion_count != 0) %>% group_by(emotion_type) %>% dplyr::summarise(emotion_count = n()) %>% arrange(-emotion_count)

emotionfilter4 <- emotion4 %>% filter(emotion_count != 0) %>% group_by(emotion_type) %>% dplyr::summarise(emotion_count = n()) %>% arrange(-emotion_count)

emotionfilter1$emotion_type <- reorder(emotionfilter1$emotion_type, emotionfilter1$emotion_count)
emotionfilter2$emotion_type <- reorder(emotionfilter2$emotion_type, emotionfilter2$emotion_count)
emotionfilter3$emotion_type <- reorder(emotionfilter3$emotion_type, emotionfilter3$emotion_count)
emotionfilter4$emotion_type <- reorder(emotionfilter4$emotion_type, emotionfilter4$emotion_count)

par(mfrow = c(2,2))
ggplot(emotionfilter, aes(x = emotion_type, y = emotion_count)) +
  geom_bar(stat = "identity") +
  labs(title = "Emotion instances, 2016 - 2018", x = "Type of Emotion", y = "Frequency") +
  theme_minimal() + coord_flip()

ggplot(emotionfilter2, aes(x = emotion_type, y = emotion_count)) +
  geom_bar(stat = "identity") +
  labs(title = "Emotion instances, 2018 - 2020", x = "Type of Emotion", y = "Frequency") +
  theme_minimal() + coord_flip()

ggplot(emotionfilter3, aes(x = emotion_type, y = emotion_count)) +
  geom_bar(stat = "identity") +
  labs(title = "Emotion instances, 2020 - 2021", x = "Type of Emotion", y = "Frequency") +
  theme_minimal() + coord_flip()

ggplot(emotionfilter4, aes(x = emotion_type, y = emotion_count)) +
  geom_bar(stat = "identity") +
  labs(title = "Emotion instances, 2021 - 2023", x = "Type of Emotion", y = "Frequency") +
  theme_minimal() + coord_flip()

```

```{r}
#Graveyard
df_renamed <- customstop_words %>% rename(word = `c("brexit", "uk", "eu", "gt", "https", "1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "100", "52", "12", "i.e", "70")`) 
anti_join(df_renamed, by = "word", copy = TRUE)

#Getting title of page
link <- "https://www.reddit.com/r/unitedkingdom/comments/kkq7a4/its_time_to_leave_the_word_brexit_behind_and_hold/" 
gettitle <- function(link) {
  response <- GET(paste0(link,".json"), user_agent("my_bot/0.1"))  # send a GET request to the page
  data <- content(response, "text")
  json_data <- fromJSON(data) 
  title <- json_data$data$children
  return(title)
}

#make make a separate dataframe with the comments,
x_list <- list(top_Brexit_urls$title,top_Brexit_urls$comments)
x<- top_Brexit_urls$title*top_Brexit_urls$comments
puttitle <- function(x_list) {
  rep<- as.data.frame(rep(x_list[[1]], x_list[[2]]))
  return(rep)
}
try <- sapply(list(top_Brexit_urls$title,top_Brexit_urls$comments), puttitle)

A <- list(top_Brexit_urls$title)
B <- list(top_Brexit_urls$comments)
result <- as.data.frame(unlist(mapply(rep, A, B)))

#Including all pairwise 
df_pairwise <- df %>% tibble() %>% mutate(document = 1:nrow(df)) %>% unnest_tokens(word,comment) %>% filter(!word %in% stop_words$word)

set.seed(123)

df_pairwise %>%
  group_by(word) %>%
  filter(n() >= 20) %>%
  pairwise_cor(word, document) %>%
  filter(!is.na(correlation),
         correlation > 0.5) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
  geom_node_point(color = "lightblue", size = 1) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()
```

```{r}
# Load required packages
library(tm)
library(stringr)

# Create TDMs
corpus1 <- Corpus(VectorSource(disgust1))
corpus2 <- Corpus(VectorSource(gen1))
tdm1 <- DocumentTermMatrix(disgust1)
tdm2 <- DocumentTermMatrix(gen1)

# Convert TDMs to matrices
mat1 <- as.matrix(tdm1)
mat2 <- as.matrix(tdm2)

# Calculate word frequencies
freq1 <- colSums(mat1)
freq2 <- colSums(mat2)

# Compare word frequencies
diff_freq <- freq2 - freq1
ratio_freq <- freq2 / freq1

# Sort by absolute differences or ratios
sorted_diff_freq <- sort(abs(diff_freq), decreasing = TRUE)
sorted_ratio_freq <- sort(ratio_freq, decreasing = TRUE)

```

