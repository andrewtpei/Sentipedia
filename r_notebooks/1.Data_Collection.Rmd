```{r}
#Relevant Packages: 
library(jsonlite);library(RedditExtractoR);library(tidyverse);library(purrr);library(tidytext); library(wordcloud2); library(textdata); library(ggthemes); library(syuzhet); library(widyr)
```

OBTAINING AND FILTERING DATA: 
```{r}
#Returns a dataframe with the threads and basic info (e.g comments, date of post, content of post, and crucially the comment url)
top_Brexit_urls <- find_thread_urls(subreddit = "unitedkingdom", keywords = "Brexit", sort_by = "top", period = "all");
top_Brexit_urls$date_utc <- as.Date(top_Brexit_urls$date_utc)
Atop_Brexit_urls <- arrange(top_Brexit_urls, date_utc)
dim(Atop_Brexit_urls); summary(Atop_Brexit_urls); head(Atop_Brexit_urls)
#Returns the comments within the threads 
threads_contents <- get_thread_content(Atop_Brexit_urls$url)
```

```{r}
#Cleaning of the two main dataframes, df_Brexit and threads_content 
df_Brexit <- threads_contents[["comments"]] %>% select(-c("author", "upvotes", "downvotes")) %>% filter(!str_detect(comment, regex("deleted", ignore_case = TRUE)))
dim(df_Brexit); summary(df_Brexit); head(df_Brexit)
filtered <- df_Brexit %>% filter(str_detect(comment, regex("Brexit", ignore_case = TRUE)));dim(filtered); summary(filtered)
dim(filtered); summary(filtered); summary(filtered)
```

```{r}
#News Article Titles over Time
Atop_Brexit_urls1 <- rename(Atop_Brexit_urls, "Reddit Link" = url)
Brexit_titles_time<- left_join(df_news,Atop_Brexit_urls1, by = "Reddit Link")
Brexit_titles_timefiltered <- filter(Brexit_titles_time, !is.na(`News Link`) & !is.na(timestamp))

```

```{r}
write.csv2(df_Brexit, "reddit_full.csv")
write.csv2(filtered,"reddit_Brexit.csv")
write.csv2(Brexit_titles_timefiltered, "Brexitnewsonly.csv", row.names = FALSE)
```

