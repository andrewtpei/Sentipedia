```{r}
#Packages: 
library(jsonlite);library(RedditExtractoR);library(tidyverse);library(purrr);library(tidytext); library(wordcloud2); library(textdata); library(ggthemes); library(syuzhet); library(widyr)
```

OBTAINING AND FILTERING DATA: 
```{r}
#Returns a dataframe with the threads and basic info (e.g comments, date of post, content of post, and crucially the comment url)
top_Brexit_urls <- find_thread_urls(subreddit = "unitedkingdom", keywords = "Brexit", sort_by = "top", period = "all");
top_Brexit_urls$date_utc <- as.Date(top_Brexit_urls$date_utc)
Atop_Brexit_urls <- arrange(top_Brexit_urls, date_utc)
dim(Atop_Brexit_urls); summary(Atop_Brexit_urls); head(Atop_Brexit_urls)
#Returns the comments within the threads 
threads_contents <- get_thread_content(Atop_Brexit_urls$url)
```

```{r}
#Cleaning: 
df_Brexit <- threads_contents[["comments"]] %>% select(-c("author", "upvotes", "downvotes")) %>% filter(!str_detect(comment, regex("deleted", ignore_case = TRUE)))
dim(df_Brexit); summary(df_Brexit); head(df_Brexit)
filtered <- df_Brexit %>% filter(str_detect(comment, regex("Brexit", ignore_case = TRUE)));dim(filtered); summary(filtered)
dim(filtered); summary(filtered); summary(filtered)

```
```{r}
##########################################################################
##########################################################################
#Create bigram network diagrams 
##########################################################################
##########################################################################
```

```{r}
df_bigrams <- df %>%
  unnest_tokens(bigram, comment, token = "ngrams", n = 2) %>%
  filter(!is.na(bigram))
bigrams_separated <- df_bigrams %>% separate(bigram, c("word1", "word2"), sep = " ")
custom_stopwords <- c("http", "amp", "https", "gt", "xit", "news")
```

```{r}
#Just emotion
bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>% 
  filter(word1 %in% sentiments$word) %>% 
  filter(word2 %in% sentiments$word) %>% 
  filter(!word1 %in% custom_stopwords) %>% 
  filter(!word2 %in% custom_stopwords) 
  

bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")
```

```{r}
#Just emotion
bigram_graph <- bigrams_united %>% count(bigram, sort = T) %>% filter(n > 10) %>% graph_from_data_frame()

set.seed(123)
bidiagram<- ggraph(bigram_graph, layout = "fr") +
        geom_edge_link() +
        geom_node_point(color = "lightblue", size = 5) +
        geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
        theme_void()
bidiagram
```

```{r}
#Including all
bigrams_filtered_all <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>% 
  filter(!word1 %in% custom_stopwords) %>% 
  filter(!word2 %in% custom_stopwords) 

bigrams_united_all <- bigrams_filtered_all %>%
  unite(bigram, word1, word2, sep = " ")
```
emotion5 <- emotion1 %>% dplyr::filter(emotion_count > 0)
```{r}
#Including all
bigram_graph_all <- bigrams_united_all %>% count(bigram, sort = T) %>% filter(n > 150) %>% graph_from_data_frame()

set.seed(123)
bidiagram_all<- ggraph(bigram_graph_all, layout = "fr") +
        geom_edge_link() +
        geom_node_point(color = "lightblue", linewidth = 1) +
        geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
        theme_void()
bidiagram_all
```

```{r}
##########################################################################
##########################################################################
#Word Combination
##########################################################################
##########################################################################
```

```{r}
#Word Correlation: 
df_wordcorr <- df %>% tibble() %>% mutate(document = 1:nrow(df)) %>% unnest_tokens(word,comment) %>% left_join(sentiments, by = "word") %>% filter(value != 0 | word == "brexit" | word == "referendum" | word == "government" | word == "labour" | word == "tory" | word == "economy" | word == "politicians" | word == "ireland" | word == "scotland" | word == "hard") %>% anti_join(stop_words, by = "word") 

word_corr <- df_wordcorr %>% widyr::pairwise_cor(word, document, sort = TRUE) 

brexit <- word_corr %>% filter(item1 == "brexit" | item1 == "referendum" | item1 == "government" | item1 == "labour" | item1 == "tory" | item1 == "economy" | item1 == "politicians" | item1 == "ireland" | item1 == "scotland" | item1 == "hard") %>% filter(n > 100) %>% graph_from_data_frame()

brexit <- word_pairs %>% filter(item1 == "brexit" | item1 == "referendum" | item1 == "government" | item1 == "labour" | item1 == "tory" | item1 == "economy" | item1 == "politicians" | item1 == "ireland" | item1 == "scotland" | item1 == "hard") %>% filter(n > 100) %>% graph_from_data_frame()

brexit2 <- word_pairs %>% filter(item1 == "brexit") %>% graph_from_data_frame()

brexit2 <- word_pairs %>% filter(item1 == "brexit") %>% graph_from_data_frame()

set.seed(123)
a <- grid::arrow(type = "closed", length = unit(.05, "inches"))
b<- ggraph(government, layout = "fr") +
        geom_edge_link() +
        geom_node_point(color = "lightblue", size = 1) +
        geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
        theme_void()

```

```{r}
##Word Count Diagram with Brexit and Brexit-related topics
df_wordcorr <- df %>% tibble() %>% mutate(document = 1:nrow(df)) %>% unnest_tokens(word,comment) %>% left_join(sentiments, by = "word") %>% filter(value != 0 | word == "brexit" | word == "government" | word == "labour" | word == "tory" | word == "economy" | word == "politicians" | word == "ireland" | word == "scotland" | word == "hard") %>% anti_join(stop_words, by = "word") 

word_pairs <- df_wordcorr %>% widyr::pairwise_count(word, document, sort = TRUE) 

brexit <- word_pairs %>% filter(n > 100) %>% graph_from_data_frame()

set.seed(123)
brexittopicgraph<- ggraph(brexit, layout = "fr") +
        geom_edge_link(alpha = 0.2) +
        geom_node_point(color = "lightblue", size = 3) +
        geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
        theme_void()
brexittopicgraph
```

```{r}
#Graveyard
group_by(Year) %>% top_n(3, rowSum) %>% ungroup() %>% Date = as.Date(Atop_Brexit_urls$date_utc)
df_renamed <- customstop_words %>% rename(word = `c("brexit", "uk", "eu", "gt", "https", "1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "100", "52", "12", "i.e", "70")`) 
anti_join(df_renamed, by = "word", copy = TRUE)

#Getting title of page
link <- "https://www.reddit.com/r/unitedkingdom/comments/kkq7a4/its_time_to_leave_the_word_brexit_behind_and_hold/" 
gettitle <- function(link) {
  response <- GET(paste0(link,".json"), user_agent("my_bot/0.1"))  # send a GET request to the page
  data <- content(response, "text")
  json_data <- fromJSON(data) 
  title <- json_data$data$children
  return(title)
}

#make make a separate dataframe with the comments,
x_list <- list(top_Brexit_urls$title,top_Brexit_urls$comments)
x<- top_Brexit_urls$title*top_Brexit_urls$comments
puttitle <- function(x_list) {
  rep<- as.data.frame(rep(x_list[[1]], x_list[[2]]))
  return(rep)
}
try <- sapply(list(top_Brexit_urls$title,top_Brexit_urls$comments), puttitle)

A <- list(top_Brexit_urls$title)
B <- list(top_Brexit_urls$comments)
result <- as.data.frame(unlist(mapply(rep, A, B)))

#Including all pairwise 
df_pairwise <- df %>% tibble() %>% mutate(document = 1:nrow(df)) %>% unnest_tokens(word,comment) %>% filter(!word %in% stop_words$word)

set.seed(123)

df_pairwise %>%
  group_by(word) %>%
  filter(n() >= 20) %>%
  pairwise_cor(word, document) %>%
  filter(!is.na(correlation),
         correlation > 0.5) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
  geom_node_point(color = "lightblue", size = 1) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()
```

```{r}
##Wordclouds Graveyard
text <- filtered %>% tibble() %>% unnest_tokens(word,comment) %>% left_join(parts_of_speech) %>% filter(!is.na(pos)) %>% filter(pos %in% c("Adjective","Verb")) %>% anti_join(stop_words, by = "word") %>% anti_join(dfcustomstop_words, by = "word") %>% left_join(sentiments, by = "word") %>%
  filter(!is.na(value) & value != 0) %>% arrange(timestamp) %>% count(word, sort = T) %>% rename(freq = n)
```

```{r}
#Words to filter out
word <- c("British", "pro", "left", "right", "anti", "leave","hard", "yeah")
dfcustomstop_words <- as.data.frame(word)
sentiments <- get_sentiments("afinn")

texttopic <- df %>% tibble() %>% unnest_tokens(word,comment) %>% left_join(parts_of_speech) %>% filter(!pos %in% c("Adjective","Verb", "Adverb", "Verb (transitive)", "Verb (intransitive)")) %>% anti_join(stop_words, by = "word") %>% arrange(timestamp)

n <- nrow(texttopic)
texttopic1 <- texttopic[1:round(n/4), ]
texttopic2 <- texttopic[(round(n/4) + 1):round(n/2), ]
texttopic3 <- texttopic[(round(n/2) + 1):round(3*n/4), ]
texttopic4 <- texttopic[(round(3*n/4) + 1):n, ]

set.seed(123)
cloudtopic1 <- texttopic1 %>% count(word, sort = T) %>% rename(freq = n);cloudtopic2 <- texttopic2 %>% count(word, sort = T) %>% rename(freq = n);cloudtopic3 <- texttopic3 %>% count(word, sort = T) %>% rename(freq = n);cloudtopic4 <- texttopic4 %>% count(word, sort = T) %>% rename(freq = n)
wordcloud2(cloudtopic1, shape = "circle");wordcloud2(cloudtopic2, shape = "circle"); wordcloud2(cloudtopic3, shape = "circle"); wordcloud2(cloudtopic4, shape = "circle")
```

```{r}
#Average Sentiment of words over time
toptopic1 <- texttopic %>% count(word, sort = T) %>% dplyr::rename(freq = n)
toptopic2 <- texttopic %>% count(word, sort = T) %>% dplyr::rename(freq = n)
toptopic3 <- texttopic %>% count(word, sort = T) %>% dplyr::rename(freq = n)
toptopic4 <- texttopic %>% count(word, sort = T) %>% dplyr::rename(freq = n)
```

```{r}
##Word Clouds Emotions
word <- c("British", "pro", "left", "right", "anti", "leave","hard", "yeah", "Brexit"); dfcustomstop_words <- as.data.frame(word)
sentiments <- get_sentiments("afinn")

text <- df %>% tibble() %>% unnest_tokens(word,comment) %>% left_join(sentiments, by = "word") %>% filter(!is.na(value) & value != 0) %>% anti_join(stop_words, by = "word") %>% anti_join(dfcustomstop_words, by = "word") %>% arrange(timestamp)

n <- nrow(text)
text1 <- text[1:round(n/4), ]
text2 <- text[(round(n/4) + 1):round(n/2), ]
text3 <- text[(round(n/2) + 1):round(3*n/4), ]
text4 <- text[(round(3*n/4) + 1):n, ]

cloud1 <- text1 %>% count(word, sort = T) %>% rename(freq = n);cloud2 <- text2 %>% count(word, sort = T) %>% rename(freq = n);cloud3 <- text3 %>% count(word, sort = T) %>% rename(freq = n);cloud4 <- text4 %>% count(word, sort = T) %>% rename(freq = n)
wordcloud2(cloud1, shape = "circle");wordcloud2(cloud2, shape = "circle"); wordcloud2(cloud3, shape = "circle"); wordcloud2(cloud4, shape = "circle")
```
